{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc7b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9b5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(data, prompt):\n",
    "    data = data[data[\"PromptType\"]==prompt]\n",
    "    return np.array([[len(data[data[\"model_bruteforce\"]==\"1\"][data[\"human_bruteforce\"]==\"1\"]), len(data[data[\"model_bruteforce\"]==\"1\"][data[\"human_bruteforce\"]==\"0\"])], [len(data[data[\"model_bruteforce\"]==\"0\"][data[\"human_bruteforce\"]==\"1\"]), len(data[data[\"model_bruteforce\"]==\"0\"][data[\"human_bruteforce\"]==\"0\"])]])\n",
    "\n",
    "def correctness(data, prompt):\n",
    "    data = data[data[\"PromptType\"]==prompt]\n",
    "    return len(data[data[\"correctness\"]==\"1\"]) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac9523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 0, 11: 0, 12: 1, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 1, 28: 0, 29: 0, 30: 0, 31: 0, 32: 1, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 1, 39: 0, 40: 0, 41: 0, 42: 0, 43: 1, 44: 1, 45: 1, 46: 1, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 1, 55: 1, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 1, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 1, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 1, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 1, 93: 0, 94: 0, 95: 0, 96: 0, 97: 1, 98: 0, 99: 0, 100: 0, 101: 0, 102: 0, 103: 1, 104: 1, 105: 0, 106: 0, 107: 1, 108: 0, 109: 0, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 0, 120: 0, 121: 0, 122: 0, 123: 0, 124: 0, 125: 0, 126: 0, 127: 0, 128: 0, 129: 1, 130: 0, 131: 0, 132: 1, 133: 1, 134: 0, 135: 1, 136: 0, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 0, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 1, 151: 0, 152: 0, 153: 0, 154: 1, 155: 0, 156: 0, 157: 0, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 0, 168: 0, 169: 0, 170: 1, 171: 0, 172: 0, 173: 1, 174: 0, 175: 0, 176: 0, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 1, 185: 0, 186: 1, 187: 0, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 0, 194: 0, 195: 0, 196: 1, 197: 0, 198: 0, 199: 0, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 0, 207: 1, 208: 0, 209: 0, 210: 0, 211: 0, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 0, 225: 1, 226: 0, 227: 0, 228: 0, 229: 0, 230: 0, 231: 0, 232: 0, 233: 0, 234: 0, 235: 0, 236: 0, 237: 0, 238: 0, 239: 0, 240: 0, 241: 0, 242: 0, 243: 0, 244: 0, 245: 0, 246: 1, 247: 0, 248: 0, 249: 0}\n",
      "{0: 0, 1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 1, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 1, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 1, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 1, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 1, 51: 0, 52: 1, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 1, 59: 0, 60: 0, 61: 0, 62: 0, 63: 1, 64: 1, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 1, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 1, 97: 0, 98: 0, 99: 0, 100: 0, 101: 0, 102: 0, 103: 0, 104: 1, 105: 0, 106: 0, 107: 1, 108: 0, 109: 1, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 0, 117: 0, 118: 0, 119: 0, 120: 0, 121: 0, 122: 0, 123: 1, 124: 0, 125: 0, 126: 0, 127: 1, 128: 0, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 0, 135: 0, 136: 0, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 0, 145: 0, 146: 0, 147: 0, 148: 0, 149: 0, 150: 0, 151: 0, 152: 0, 153: 0, 154: 0, 155: 0, 156: 0, 157: 0, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 0, 165: 0, 166: 0, 167: 0, 168: 0, 169: 0, 170: 0, 171: 0, 172: 0, 173: 0, 174: 0, 175: 0, 176: 1, 177: 0, 178: 0, 179: 0, 180: 0, 181: 0, 182: 0, 183: 0, 184: 0, 185: 0, 186: 0, 187: 1, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 1, 194: 0, 195: 0, 196: 0, 197: 0, 198: 0, 199: 0, 200: 0, 201: 0, 202: 0, 203: 0, 204: 1, 205: 0, 206: 0, 207: 0, 208: 0, 209: 0, 210: 0, 211: 1, 212: 0, 213: 0, 214: 0, 215: 0, 216: 0, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 1, 223: 0, 224: 0, 225: 0, 226: 0, 227: 1, 228: 0, 229: 0, 230: 0, 231: 0, 232: 0, 233: 0, 234: 0, 235: 0, 236: 0, 237: 0, 238: 0, 239: 0, 240: 0, 241: 0, 242: 0, 243: 0, 244: 1, 245: 0, 246: 0, 247: 0, 248: 0, 249: 0}\n"
     ]
    }
   ],
   "source": [
    "alldata = {}\n",
    "models = ['DSChat', 'DSReason', 'GeminiFlash', 'o3', 'Qwen1', 'Qwen14', 'Qwen70']\n",
    "tests = ['MathMain', 'MathHint', 'MathHintCombined', 'LogicMain', 'LogicHint', 'LogicHintCombined']\n",
    "humanbruteforcemath = {}\n",
    "humanbruteforcetotalmath = {}\n",
    "humanbruteforcelogic = {}\n",
    "humanbruteforcetotallogic = {}\n",
    "mathdifficulty = {}\n",
    "logicdifficulty = {}\n",
    "mathpopularity = {}\n",
    "logicpopularity = {}\n",
    "mathcategory = {}\n",
    "logiccategory = {}\n",
    "\n",
    "logicdata = pd.read_csv(\"data/braingle/braingle_Logic.csv\")\n",
    "mathdata = pd.read_csv(\"data/braingle/braingle_Math.csv\")\n",
    "for index, row in logicdata.iterrows():\n",
    "    logicdifficulty[index] = row[\"Difficulty\"]\n",
    "    logicpopularity[index] = row[\"Popularity/Fun\"]\n",
    "    logiccategory[index] = row['Problem type [IN PROGRESS]']\n",
    "for index, row in mathdata.iterrows():\n",
    "    mathdifficulty[index] = row[\"Difficulty\"]\n",
    "    mathpopularity[index] = row[\"Popularity/Fun\"]\n",
    "    # mathcategory[index] = row['Problem type [IN PROGRESS]']\n",
    "\n",
    "# models = ['FinalLogic-Qwen1']\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/HintRedo-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"LogicHint\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"LogicHint\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "            humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/FinalLogic-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"LogicMain\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"LogicMain\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "            humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/FinalMath-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"MathMain\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"MathMain\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                humanbruteforcemath[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "            humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/HintRedo-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"MathHint\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"MathHint\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                humanbruteforcemath[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "            humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/CombinedHint-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            alldata[(model, \"LogicHintCombined\")] = [json.loads(line) for line in file]\n",
    "\n",
    "        for index, row in pd.DataFrame(alldata[(model, \"LogicHintCombined\")]).iterrows():\n",
    "            try:\n",
    "                if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                    continue\n",
    "                if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                    humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                    humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "                humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/CombinedHint-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            alldata[(model, \"MathHintCombined\")] = [json.loads(line) for line in file]\n",
    "\n",
    "        for index, row in pd.DataFrame(alldata[(model, \"MathHintCombined\")]).iterrows():\n",
    "            try:\n",
    "                if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                    continue\n",
    "                if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                    humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                    humanbruteforcemath[row[\"ID\"]] = 0\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "                humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "for i in range(250):\n",
    "    humanbruteforcemath[i] = round(humanbruteforcemath[i] / humanbruteforcetotalmath[i])\n",
    "    humanbruteforcelogic[i] = round(humanbruteforcelogic[i] / humanbruteforcetotallogic[i])\n",
    "\n",
    "print(humanbruteforcemath)\n",
    "print(humanbruteforcelogic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d03c56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DSChat, Test: MathMain, Prompt: basicprompt [10.4 30.8  2.8 56. ] 250.0 Correctness: 0.58/0.4\n",
      "Model: DSChat, Test: MathMain, Prompt: mathPrompt [ 9.6 28.4  3.6 58.4] 250.0 Correctness: 0.536/0.32\n",
      "Model: DSChat, Test: MathHint, Prompt: hintPrompt [ 9.6 28.8  3.6 58. ] 250.0 Correctness: 0.568/0.38\n",
      "Model: DSChat, Test: MathHintCombined, Prompt: combinedhintPrompt [11.2 30.   2.  56.8] 250.0 Correctness: 0.6/0.38\n",
      "Model: DSChat, Test: LogicMain, Prompt: basicprompt [ 6. 38.  4. 52.] 250.0 Correctness: 0.392/0.32\n",
      "Model: DSChat, Test: LogicMain, Prompt: mathPrompt [ 7.2 34.   2.8 56. ] 250.0 Correctness: 0.412/0.34\n",
      "Model: DSChat, Test: LogicHint, Prompt: hintPrompt [ 7.6 30.   2.4 60. ] 250.0 Correctness: 0.414/0.26\n",
      "Model: DSChat, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 6.4 28.8  3.6 61.2] 250.0 Correctness: 0.408/0.2\n",
      "Model: DSReason, Test: MathMain, Prompt: basicprompt [ 8.  13.2  5.2 73.6] 250.0 Correctness: 0.664/0.48\n",
      "Model: DSReason, Test: MathMain, Prompt: mathPrompt [ 8.8 10.8  4.4 75.9] 249.0 Correctness: 0.677/0.52\n",
      "Model: DSReason, Test: MathHint, Prompt: hintPrompt [ 8.8 15.6  4.4 71.2] 250.0 Correctness: 0.728/0.48\n",
      "Model: DSReason, Test: MathHintCombined, Prompt: combinedhintPrompt [ 8.4 10.8  4.8 76. ] 250.0 Correctness: 0.724/0.6\n",
      "Model: DSReason, Test: LogicMain, Prompt: basicprompt [ 5.2 13.3  4.4 77.1] 249.0 Correctness: 0.446/0.26\n",
      "Model: DSReason, Test: LogicMain, Prompt: mathPrompt [ 4. 14.  6. 76.] 250.0 Correctness: 0.44/0.28\n",
      "Model: DSReason, Test: LogicHint, Prompt: hintPrompt [ 5.2 10.4  4.4 79.9] 249.0 Correctness: 0.498/0.347\n",
      "Model: DSReason, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 6.   6.   3.6 84.3] 249.0 Correctness: 0.526/0.44\n",
      "Model: GeminiFlash, Test: MathMain, Prompt: basicprompt [ 7.8 16.7  5.3 70.2] 245.0 Correctness: 0.678/0.5\n",
      "Model: GeminiFlash, Test: MathMain, Prompt: mathPrompt [ 7.  15.2  6.6 71.3] 244.0 Correctness: 0.687/0.479\n",
      "Model: GeminiFlash, Test: MathHint, Prompt: hintPrompt [ 1.2  7.4  2.5 88.9] 81.0 Correctness: 0.827/0.6\n",
      "Model: GeminiFlash, Test: MathHintCombined, Prompt: combinedhintPrompt [ 1.3  0.   2.7 96. ] 75.0 Correctness: 0.707/0.571\n",
      "Model: GeminiFlash, Test: LogicMain, Prompt: basicprompt [ 7.6 15.3  2.4 74.7] 249.0 Correctness: 0.141/0.02\n",
      "Model: GeminiFlash, Test: LogicMain, Prompt: mathPrompt [ 8.1 11.7  2.  78.1] 247.0 Correctness: 0.108/0.044\n",
      "Model: GeminiFlash, Test: LogicHint, Prompt: hintPrompt [ 4.9  8.2  3.3 83.6] 61.0 Correctness: 0.508/0.2\n",
      "Model: GeminiFlash, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 1.8 12.5  1.8 83.9] 56.0 Correctness: 0.518/0.0\n",
      "Model: o3, Test: MathMain, Prompt: basicprompt [ 7.3  9.8  6.1 76.8] 246.0 Correctness: 0.809/0.667\n",
      "Model: o3, Test: MathMain, Prompt: mathPrompt [ 4.8  4.4  8.5 82.3] 248.0 Correctness: 0.798/0.66\n",
      "Model: o3, Test: MathHint, Prompt: hintPrompt [ 4.9  8.9  6.4 79.8] 203.0 Correctness: 0.901/0.806\n",
      "Model: o3, Test: MathHintCombined, Prompt: combinedhintPrompt [ 3.   4.6  7.1 85.3] 197.0 Correctness: 0.904/0.806\n",
      "Model: o3, Test: LogicMain, Prompt: basicprompt [ 2.3  8.9  7.5 81.3] 214.0 Correctness: 0.827/0.824\n",
      "Model: o3, Test: LogicMain, Prompt: mathPrompt [ 1.8  5.   6.8 86.4] 220.0 Correctness: 0.782/0.694\n",
      "Model: o3, Test: LogicHint, Prompt: hintPrompt [ 2.3  7.   6.4 84.3] 172.0 Correctness: 0.878/0.818\n",
      "Model: o3, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 4.3  9.8  5.5 80.5] 164.0 Correctness: 0.915/0.952\n",
      "Model: Qwen1, Test: MathMain, Prompt: basicprompt [ 9.2 28.4  4.  58.4] 250.0 Correctness: 0.164/0.12\n",
      "Model: Qwen1, Test: MathMain, Prompt: mathPrompt [ 9.2 30.   4.  56.8] 250.0 Correctness: 0.16/0.08\n",
      "Model: Qwen1, Test: MathHint, Prompt: hintPrompt [ 8.8 31.2  4.4 55.6] 250.0 Correctness: 0.148/0.08\n",
      "Model: Qwen1, Test: MathHintCombined, Prompt: combinedhintPrompt [ 9.6 28.8  3.6 58. ] 250.0 Correctness: 0.192/0.1\n",
      "Model: Qwen1, Test: LogicMain, Prompt: basicprompt [ 6.4 22.4  3.6 67.6] 250.0 Correctness: 0.04/0.04\n",
      "Model: Qwen1, Test: LogicMain, Prompt: mathPrompt [ 6.4 21.6  3.6 68.4] 250.0 Correctness: 0.036/0.04\n",
      "Model: Qwen1, Test: LogicHint, Prompt: hintPrompt [ 6.4 16.8  3.6 73.2] 250.0 Correctness: 0.056/0.06\n",
      "Model: Qwen1, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 8.4 22.8  1.6 67.2] 250.0 Correctness: 0.044/0.06\n",
      "Model: Qwen14, Test: MathMain, Prompt: basicprompt [10.  33.6  3.2 53.2] 250.0 Correctness: 0.424/0.24\n",
      "Model: Qwen14, Test: MathMain, Prompt: mathPrompt [10.8 27.2  2.4 59.6] 250.0 Correctness: 0.412/0.24\n",
      "Model: Qwen14, Test: MathHint, Prompt: hintPrompt [10.4 28.4  2.8 58.4] 250.0 Correctness: 0.436/0.2\n",
      "Model: Qwen14, Test: MathHintCombined, Prompt: combinedhintPrompt [ 9.6 28.4  3.6 58.4] 250.0 Correctness: 0.424/0.24\n",
      "Model: Qwen14, Test: LogicMain, Prompt: basicprompt [ 7.2 35.6  2.8 54.4] 250.0 Correctness: 0.22/0.16\n",
      "Model: Qwen14, Test: LogicMain, Prompt: mathPrompt [ 6.4 34.8  3.6 55.2] 250.0 Correctness: 0.228/0.16\n",
      "Model: Qwen14, Test: LogicHint, Prompt: hintPrompt [ 7.2 32.4  2.8 57.6] 250.0 Correctness: 0.284/0.22\n",
      "Model: Qwen14, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 5.2 33.2  4.8 56.8] 250.0 Correctness: 0.26/0.28\n",
      "Model: Qwen70, Test: MathMain, Prompt: basicprompt [11.6 27.2  1.6 59.6] 250.0 Correctness: 0.42/0.18\n",
      "Model: Qwen70, Test: MathMain, Prompt: mathPrompt [ 9.6 27.2  3.6 59.6] 250.0 Correctness: 0.404/0.22\n",
      "Model: Qwen70, Test: MathHint, Prompt: hintPrompt [ 9.2 25.6  4.  61.2] 250.0 Correctness: 0.448/0.24\n",
      "Model: Qwen70, Test: MathHintCombined, Prompt: combinedhintPrompt [ 8.8 26.   4.4 60.8] 250.0 Correctness: 0.428/0.18\n",
      "Model: Qwen70, Test: LogicMain, Prompt: basicprompt [ 8.  26.8  2.  63.2] 250.0 Correctness: 0.248/0.18\n",
      "Model: Qwen70, Test: LogicMain, Prompt: mathPrompt [ 7.2 25.6  2.8 64.4] 250.0 Correctness: 0.252/0.16\n",
      "Model: Qwen70, Test: LogicHint, Prompt: hintPrompt [ 6.4 25.6  3.6 64.4] 250.0 Correctness: 0.272/0.2\n",
      "Model: Qwen70, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 7.2 23.2  2.8 66.8] 250.0 Correctness: 0.301/0.265\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for test in tests:\n",
    "        try:\n",
    "            data = pd.DataFrame(alldata[(model, test)])\n",
    "        except:\n",
    "            continue\n",
    "        prompts = data[\"PromptType\"].unique()\n",
    "\n",
    "        for prompt in prompts:\n",
    "            if \"hint\" in prompt and \"Main\" in test or \"symbol\" in prompt:\n",
    "                continue\n",
    "            tempdata = data[data[\"PromptType\"]==prompt]\n",
    "            bfarray = np.zeros((2, 2))\n",
    "            bfdiff = []\n",
    "            nbfdiff = []\n",
    "            bfpop = []\n",
    "            nbfpop = []\n",
    "            correctness = []\n",
    "            correctnessDiff = []\n",
    "            category = {}\n",
    "            count = 0\n",
    "\n",
    "            for index, row in tempdata.iterrows():\n",
    "                # if (type(row[\"Response\"]) != str):\n",
    "                #     print(row[\"Response\"])\n",
    "                if (type(row[\"Response\"]) != str or row[\"Response\"] == None or row[\"Response\"] == \"NaN\" or row[\"Response\"] == \"None\" or row[\"Response\"] == \"\" or row[\"model_bruteforce\"] == \"NULL\" or row[\"Response\"] is str and row[\"Response\"].isspace()):\n",
    "                    # print(\"Filtered!\")\n",
    "                    continue\n",
    "                try:\n",
    "                    if (row[\"model_bruteforce\"] == \"1\"):\n",
    "                        if \"Math\" in test:\n",
    "                            bfdiff.append(mathdifficulty[row[\"ID\"]])\n",
    "                            bfpop.append(mathpopularity[row[\"ID\"]])\n",
    "                        else:\n",
    "                            bfdiff.append(logicdifficulty[row[\"ID\"]])\n",
    "                            bfpop.append(logicpopularity[row[\"ID\"]])\n",
    "                    elif (row[\"model_bruteforce\"] == \"0\"):\n",
    "                        if \"Math\" in test:\n",
    "                            nbfdiff.append(mathdifficulty[row[\"ID\"]])\n",
    "                            nbfpop.append(mathpopularity[row[\"ID\"]])\n",
    "                        else:\n",
    "                            nbfdiff.append(logicdifficulty[row[\"ID\"]])\n",
    "                            nbfpop.append(logicpopularity[row[\"ID\"]])\n",
    "                    \n",
    "                    if \"Math\" in test:\n",
    "                        bfarray[1-int(row[\"model_bruteforce\"])][1-humanbruteforcemath[row['ID']]] += 1\n",
    "                    if \"Logic\" in test:\n",
    "                        bfarray[1-int(row[\"model_bruteforce\"])][1-humanbruteforcelogic[row['ID']]] += 1\n",
    "                    correctness.append(int(row[\"correctness\"]))\n",
    "\n",
    "                    if \"Logic\" in test:\n",
    "                        # print(logiccategory[row[\"ID\"]])\n",
    "                        if logiccategory[row[\"ID\"]] not in category.keys():\n",
    "                            category[logiccategory[row[\"ID\"]]] = [0, 0]\n",
    "                        \n",
    "                        # print(row[\"model_bruteforce\"])\n",
    "\n",
    "                        category[logiccategory[row[\"ID\"]]][1-int(row[\"model_bruteforce\"])] += 1\n",
    "\n",
    "                    if (row['ID'] < 50):\n",
    "                        correctnessDiff.append(int(row[\"correctness\"]))\n",
    "                    # count += 1\n",
    "\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # print(\"Error:\", e)\n",
    "                    # print(row[\"model_bruteforce\"])\n",
    "                    pass\n",
    "            # print(category)\n",
    "            \n",
    "            print(f\"Model: {model}, Test: {test}, Prompt: {prompt}\", \n",
    "                #   print(category),\n",
    "                  np.round(100*bfarray.flatten()/np.sum(bfarray), 1), np.sum(bfarray), \n",
    "                  \"Correctness:\", str(np.round(np.mean(correctness), 3)) + \"/\" + str(np.round(np.mean(correctnessDiff), 3)), \n",
    "                #   \"Difficulty (BF/NBF):\", np.round(np.mean(bfdiff), 2), np.round(np.mean(nbfdiff), 2), \n",
    "                #   \"Popularity (BF/NBF):\", np.round(np.mean(bfpop), 2), np.round(np.mean(nbfpop), 2),\n",
    "                  \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ed10e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Solution Math:\n",
      "Difficulty (BF/NBF): 2.83 2.8\n",
      "Popularity (BF/NBF): 2.32 2.33\n",
      "Human Solution Logic:\n",
      "Difficulty (BF/NBF): 2.7 2.65\n",
      "Popularity (BF/NBF): 2.37 2.51\n"
     ]
    }
   ],
   "source": [
    "print(\"Human Solution Math:\")\n",
    "totaldiffbf = []\n",
    "totaldiffnbf = []\n",
    "totalpopbf = []\n",
    "totalpopnbf = []\n",
    "for i in range(250):\n",
    "    if (humanbruteforcemath[i] == 1):\n",
    "        totaldiffbf.append(mathdifficulty[i])\n",
    "        totalpopbf.append(mathpopularity[i])\n",
    "    else:\n",
    "        totaldiffnbf.append(mathdifficulty[i])\n",
    "        totalpopnbf.append(mathpopularity[i])\n",
    "print(\"Difficulty (BF/NBF):\", np.round(np.mean(totaldiffbf), 2), np.round(np.mean(totaldiffnbf), 2))\n",
    "print(\"Popularity (BF/NBF):\", np.round(np.mean(totalpopbf), 2), np.round(np.mean(totalpopnbf), 2))\n",
    "print(\"Human Solution Logic:\")\n",
    "totaldiffbf = []\n",
    "totaldiffnbf = []\n",
    "totalpopbf = []\n",
    "totalpopnbf = []\n",
    "for i in range(250):\n",
    "    if (humanbruteforcelogic[i] == 1):\n",
    "        totaldiffbf.append(logicdifficulty[i])\n",
    "        totalpopbf.append(logicpopularity[i])\n",
    "    else:\n",
    "        totaldiffnbf.append(logicdifficulty[i])\n",
    "        totalpopnbf.append(logicpopularity[i])\n",
    "print(\"Difficulty (BF/NBF):\", np.round(np.mean(totaldiffbf), 2), np.round(np.mean(totaldiffnbf), 2))\n",
    "print(\"Popularity (BF/NBF):\", np.round(np.mean(totalpopbf), 2), np.round(np.mean(totalpopnbf), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912abea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brainteasers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
