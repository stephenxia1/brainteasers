{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc7b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9b5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(data, prompt):\n",
    "    data = data[data[\"PromptType\"]==prompt]\n",
    "    return np.array([[len(data[data[\"model_bruteforce\"]==\"1\"][data[\"human_bruteforce\"]==\"1\"]), len(data[data[\"model_bruteforce\"]==\"1\"][data[\"human_bruteforce\"]==\"0\"])], [len(data[data[\"model_bruteforce\"]==\"0\"][data[\"human_bruteforce\"]==\"1\"]), len(data[data[\"model_bruteforce\"]==\"0\"][data[\"human_bruteforce\"]==\"0\"])]])\n",
    "\n",
    "def correctness(data, prompt):\n",
    "    data = data[data[\"PromptType\"]==prompt]\n",
    "    return len(data[data[\"correctness\"]==\"1\"]) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ac9523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = {}\n",
    "models = ['DSChat', 'DSReason', 'GeminiFlash', 'o3', 'Qwen1', 'Qwen14', 'Qwen70']\n",
    "tests = ['MathMain', 'MathHint', 'MathHintCombined', 'LogicMain', 'LogicHint', 'LogicHintCombined']\n",
    "humanbruteforcemath = {}\n",
    "humanbruteforcetotalmath = {}\n",
    "humanbruteforcelogic = {}\n",
    "humanbruteforcetotallogic = {}\n",
    "mathdifficulty = {}\n",
    "logicdifficulty = {}\n",
    "mathpopularity = {}\n",
    "logicpopularity = {}\n",
    "mathcategory = {}\n",
    "logiccategory = {}\n",
    "\n",
    "logicdata = pd.read_csv(\"data/braingle/braingle_Logic.csv\")\n",
    "mathdata = pd.read_csv(\"data/braingle/braingle_Math.csv\")\n",
    "for index, row in logicdata.iterrows():\n",
    "    logicdifficulty[index] = row[\"Difficulty\"]\n",
    "    logicpopularity[index] = row[\"Popularity/Fun\"]\n",
    "    logiccategory[index] = row['Problem type [IN PROGRESS]']\n",
    "for index, row in mathdata.iterrows():\n",
    "    mathdifficulty[index] = row[\"Difficulty\"]\n",
    "    mathpopularity[index] = row[\"Popularity/Fun\"]\n",
    "    # mathcategory[index] = row['Problem type [IN PROGRESS]']\n",
    "\n",
    "# models = ['FinalLogic-Qwen1']\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/HintRedo-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"LogicHint\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"LogicHint\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "            humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/FinalLogic-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"LogicMain\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"LogicMain\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "            humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/FinalMath-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"MathMain\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"MathMain\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                humanbruteforcemath[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "            humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/HintRedo-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        alldata[(model, \"MathHint\")] = [json.loads(line) for line in file]\n",
    "\n",
    "    for index, row in pd.DataFrame(alldata[(model, \"MathHint\")]).iterrows():\n",
    "        try:\n",
    "            if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                continue\n",
    "            if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                humanbruteforcemath[row[\"ID\"]] = 0\n",
    "            humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "            humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Logic/CombinedHint-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            alldata[(model, \"LogicHintCombined\")] = [json.loads(line) for line in file]\n",
    "\n",
    "        for index, row in pd.DataFrame(alldata[(model, \"LogicHintCombined\")]).iterrows():\n",
    "            try:\n",
    "                if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                    continue\n",
    "                if (row[\"ID\"] not in humanbruteforcetotallogic.keys()):\n",
    "                    humanbruteforcetotallogic[row[\"ID\"]] = 0\n",
    "                    humanbruteforcelogic[row[\"ID\"]] = 0\n",
    "                humanbruteforcetotallogic[row[\"ID\"]] += 1\n",
    "                humanbruteforcelogic[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"response_evaluation/Math/CombinedHint-{model}/resultsEvaluations_evaluatedbyo3-2025-04-16.jsonl\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            alldata[(model, \"MathHintCombined\")] = [json.loads(line) for line in file]\n",
    "\n",
    "        for index, row in pd.DataFrame(alldata[(model, \"MathHintCombined\")]).iterrows():\n",
    "            try:\n",
    "                if (row['human_bruteforce'] != '1' and row['human_bruteforce'] != '0'):\n",
    "                    continue\n",
    "                if (row[\"ID\"] not in humanbruteforcetotalmath.keys()):\n",
    "                    humanbruteforcetotalmath[row[\"ID\"]] = 0\n",
    "                    humanbruteforcemath[row[\"ID\"]] = 0\n",
    "                humanbruteforcetotalmath[row[\"ID\"]] += 1\n",
    "                humanbruteforcemath[row[\"ID\"]] += int(row['human_bruteforce'])\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "for i in range(250):\n",
    "    humanbruteforcemath[i] = round(humanbruteforcemath[i] / humanbruteforcetotalmath[i])\n",
    "    humanbruteforcelogic[i] = round(humanbruteforcelogic[i] / humanbruteforcetotallogic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d03c56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DSChat, Test: MathMain, Prompt: basicprompt [10.4 30.8  2.8 56. ] 250.0 Correctness: 0.58/0.4\n",
      "Model: DSChat, Test: MathMain, Prompt: mathPrompt [ 9.6 28.4  3.6 58.4] 250.0 Correctness: 0.536/0.32\n",
      "Model: DSChat, Test: MathHint, Prompt: hintPrompt [ 9.6 28.8  3.6 58. ] 250.0 Correctness: 0.568/0.38\n",
      "Model: DSChat, Test: MathHintCombined, Prompt: combinedhintPrompt [11.2 30.   2.  56.8] 250.0 Correctness: 0.6/0.38\n",
      "Model: DSChat, Test: LogicMain, Prompt: basicprompt [ 6. 38.  4. 52.] 250.0 Correctness: 0.392/0.32\n",
      "Model: DSChat, Test: LogicMain, Prompt: mathPrompt [ 7.2 34.   2.8 56. ] 250.0 Correctness: 0.412/0.34\n",
      "Model: DSChat, Test: LogicHint, Prompt: hintPrompt [ 7.6 30.   2.4 60. ] 250.0 Correctness: 0.414/0.26\n",
      "Model: DSChat, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 6.4 28.8  3.6 61.2] 250.0 Correctness: 0.408/0.2\n",
      "Model: DSReason, Test: MathMain, Prompt: basicprompt [ 8.  13.2  5.2 73.6] 250.0 Correctness: 0.664/0.48\n",
      "Model: DSReason, Test: MathMain, Prompt: mathPrompt [ 8.8 10.8  4.4 75.9] 249.0 Correctness: 0.677/0.52\n",
      "Model: DSReason, Test: MathHint, Prompt: hintPrompt [ 8.8 15.6  4.4 71.2] 250.0 Correctness: 0.728/0.48\n",
      "Model: DSReason, Test: MathHintCombined, Prompt: combinedhintPrompt [ 8.4 10.8  4.8 76. ] 250.0 Correctness: 0.724/0.6\n",
      "Model: DSReason, Test: LogicMain, Prompt: basicprompt [ 5.2 13.3  4.4 77.1] 249.0 Correctness: 0.446/0.26\n",
      "Model: DSReason, Test: LogicMain, Prompt: mathPrompt [ 4. 14.  6. 76.] 250.0 Correctness: 0.44/0.28\n",
      "Model: DSReason, Test: LogicHint, Prompt: hintPrompt [ 5.2 10.4  4.4 79.9] 249.0 Correctness: 0.498/0.347\n",
      "Model: DSReason, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 6.   6.   3.6 84.3] 249.0 Correctness: 0.526/0.44\n",
      "Model: GeminiFlash, Test: MathMain, Prompt: basicprompt [ 7.8 16.7  5.3 70.2] 245.0 Correctness: 0.678/0.5\n",
      "Model: GeminiFlash, Test: MathMain, Prompt: mathPrompt [ 7.  15.2  6.6 71.3] 244.0 Correctness: 0.687/0.479\n",
      "Model: GeminiFlash, Test: MathHint, Prompt: hintPrompt [ 1.2  7.4  2.5 88.9] 81.0 Correctness: 0.827/0.6\n",
      "Model: GeminiFlash, Test: MathHintCombined, Prompt: combinedhintPrompt [ 1.3  0.   2.7 96. ] 75.0 Correctness: 0.707/0.571\n",
      "Model: GeminiFlash, Test: LogicMain, Prompt: basicprompt [ 7.6 15.3  2.4 74.7] 249.0 Correctness: 0.141/0.02\n",
      "Model: GeminiFlash, Test: LogicMain, Prompt: mathPrompt [ 8.1 11.7  2.  78.1] 247.0 Correctness: 0.108/0.044\n",
      "Model: GeminiFlash, Test: LogicHint, Prompt: hintPrompt [ 4.9  8.2  3.3 83.6] 61.0 Correctness: 0.508/0.2\n",
      "Model: GeminiFlash, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 1.8 12.5  1.8 83.9] 56.0 Correctness: 0.518/0.0\n",
      "Model: o3, Test: MathMain, Prompt: basicprompt [ 7.3  9.8  6.1 76.8] 246.0 Correctness: 0.809/0.667\n",
      "Model: o3, Test: MathMain, Prompt: mathPrompt [ 4.8  4.4  8.5 82.3] 248.0 Correctness: 0.798/0.66\n",
      "Model: o3, Test: MathHint, Prompt: hintPrompt [ 4.9  8.9  6.4 79.8] 203.0 Correctness: 0.901/0.806\n",
      "Model: o3, Test: MathHintCombined, Prompt: combinedhintPrompt [ 3.   4.6  7.1 85.3] 197.0 Correctness: 0.904/0.806\n",
      "Model: o3, Test: LogicMain, Prompt: basicprompt [ 2.3  8.9  7.5 81.3] 214.0 Correctness: 0.827/0.824\n",
      "Model: o3, Test: LogicMain, Prompt: mathPrompt [ 1.8  5.   6.8 86.4] 220.0 Correctness: 0.782/0.694\n",
      "Model: o3, Test: LogicHint, Prompt: hintPrompt [ 2.3  7.   6.4 84.3] 172.0 Correctness: 0.878/0.818\n",
      "Model: o3, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 4.3  9.8  5.5 80.5] 164.0 Correctness: 0.915/0.952\n",
      "Model: Qwen1, Test: MathMain, Prompt: basicprompt [ 9.2 28.4  4.  58.4] 250.0 Correctness: 0.164/0.12\n",
      "Model: Qwen1, Test: MathMain, Prompt: mathPrompt [ 9.2 30.   4.  56.8] 250.0 Correctness: 0.16/0.08\n",
      "Model: Qwen1, Test: MathHint, Prompt: hintPrompt [ 8.8 31.2  4.4 55.6] 250.0 Correctness: 0.148/0.08\n",
      "Model: Qwen1, Test: MathHintCombined, Prompt: combinedhintPrompt [ 9.6 28.8  3.6 58. ] 250.0 Correctness: 0.192/0.1\n",
      "Model: Qwen1, Test: LogicMain, Prompt: basicprompt [ 6.4 22.4  3.6 67.6] 250.0 Correctness: 0.04/0.04\n",
      "Model: Qwen1, Test: LogicMain, Prompt: mathPrompt [ 6.4 21.6  3.6 68.4] 250.0 Correctness: 0.036/0.04\n",
      "Model: Qwen1, Test: LogicHint, Prompt: hintPrompt [ 6.4 16.8  3.6 73.2] 250.0 Correctness: 0.056/0.06\n",
      "Model: Qwen1, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 8.4 22.8  1.6 67.2] 250.0 Correctness: 0.044/0.06\n",
      "Model: Qwen14, Test: MathMain, Prompt: basicprompt [10.  33.6  3.2 53.2] 250.0 Correctness: 0.424/0.24\n",
      "Model: Qwen14, Test: MathMain, Prompt: mathPrompt [10.8 27.2  2.4 59.6] 250.0 Correctness: 0.412/0.24\n",
      "Model: Qwen14, Test: MathHint, Prompt: hintPrompt [10.4 28.4  2.8 58.4] 250.0 Correctness: 0.436/0.2\n",
      "Model: Qwen14, Test: MathHintCombined, Prompt: combinedhintPrompt [ 9.6 28.4  3.6 58.4] 250.0 Correctness: 0.424/0.24\n",
      "Model: Qwen14, Test: LogicMain, Prompt: basicprompt [ 7.2 35.6  2.8 54.4] 250.0 Correctness: 0.22/0.16\n",
      "Model: Qwen14, Test: LogicMain, Prompt: mathPrompt [ 6.4 34.8  3.6 55.2] 250.0 Correctness: 0.228/0.16\n",
      "Model: Qwen14, Test: LogicHint, Prompt: hintPrompt [ 7.2 32.4  2.8 57.6] 250.0 Correctness: 0.284/0.22\n",
      "Model: Qwen14, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 5.2 33.2  4.8 56.8] 250.0 Correctness: 0.26/0.28\n",
      "Model: Qwen70, Test: MathMain, Prompt: basicprompt [11.6 27.2  1.6 59.6] 250.0 Correctness: 0.42/0.18\n",
      "Model: Qwen70, Test: MathMain, Prompt: mathPrompt [ 9.6 27.2  3.6 59.6] 250.0 Correctness: 0.404/0.22\n",
      "Model: Qwen70, Test: MathHint, Prompt: hintPrompt [ 9.2 25.6  4.  61.2] 250.0 Correctness: 0.448/0.24\n",
      "Model: Qwen70, Test: MathHintCombined, Prompt: combinedhintPrompt [ 8.8 26.   4.4 60.8] 250.0 Correctness: 0.428/0.18\n",
      "Model: Qwen70, Test: LogicMain, Prompt: basicprompt [ 8.  26.8  2.  63.2] 250.0 Correctness: 0.248/0.18\n",
      "Model: Qwen70, Test: LogicMain, Prompt: mathPrompt [ 7.2 25.6  2.8 64.4] 250.0 Correctness: 0.252/0.16\n",
      "Model: Qwen70, Test: LogicHint, Prompt: hintPrompt [ 6.4 25.6  3.6 64.4] 250.0 Correctness: 0.272/0.2\n",
      "Model: Qwen70, Test: LogicHintCombined, Prompt: combinedhintPrompt [ 7.2 23.2  2.8 66.8] 250.0 Correctness: 0.301/0.265\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for test in tests:\n",
    "        try:\n",
    "            data = pd.DataFrame(alldata[(model, test)])\n",
    "        except:\n",
    "            continue\n",
    "        prompts = data[\"PromptType\"].unique()\n",
    "\n",
    "        for prompt in prompts:\n",
    "            if \"hint\" in prompt and \"Main\" in test or \"symbol\" in prompt:\n",
    "                continue\n",
    "            tempdata = data[data[\"PromptType\"]==prompt]\n",
    "            bfarray = np.zeros((2, 2))\n",
    "            bfdiff = []\n",
    "            nbfdiff = []\n",
    "            bfpop = []\n",
    "            nbfpop = []\n",
    "            correctness = []\n",
    "            correctnessDiff = []\n",
    "            category = {}\n",
    "            count = 0\n",
    "\n",
    "            for index, row in tempdata.iterrows():\n",
    "                # if (type(row[\"Response\"]) != str):\n",
    "                #     print(row[\"Response\"])\n",
    "                if (type(row[\"Response\"]) != str or row[\"Response\"] == None or row[\"Response\"] == \"NaN\" or row[\"Response\"] == \"None\" or row[\"Response\"] == \"\" or row[\"model_bruteforce\"] == \"NULL\" or row[\"Response\"] is str and row[\"Response\"].isspace()):\n",
    "                    # print(\"Filtered!\")\n",
    "                    continue\n",
    "                try:\n",
    "                    if (row[\"model_bruteforce\"] == \"1\"):\n",
    "                        if \"Math\" in test:\n",
    "                            bfdiff.append(mathdifficulty[row[\"ID\"]])\n",
    "                            bfpop.append(mathpopularity[row[\"ID\"]])\n",
    "                        else:\n",
    "                            bfdiff.append(logicdifficulty[row[\"ID\"]])\n",
    "                            bfpop.append(logicpopularity[row[\"ID\"]])\n",
    "                    elif (row[\"model_bruteforce\"] == \"0\"):\n",
    "                        if \"Math\" in test:\n",
    "                            nbfdiff.append(mathdifficulty[row[\"ID\"]])\n",
    "                            nbfpop.append(mathpopularity[row[\"ID\"]])\n",
    "                        else:\n",
    "                            nbfdiff.append(logicdifficulty[row[\"ID\"]])\n",
    "                            nbfpop.append(logicpopularity[row[\"ID\"]])\n",
    "                    \n",
    "                    if \"Math\" in test:\n",
    "                        bfarray[1-int(row[\"model_bruteforce\"])][1-humanbruteforcemath[row['ID']]] += 1\n",
    "                    if \"Logic\" in test:\n",
    "                        bfarray[1-int(row[\"model_bruteforce\"])][1-humanbruteforcelogic[row['ID']]] += 1\n",
    "                    correctness.append(int(row[\"correctness\"]))\n",
    "\n",
    "                    if \"Logic\" in test:\n",
    "                        # print(logiccategory[row[\"ID\"]])\n",
    "                        if logiccategory[row[\"ID\"]] not in category.keys():\n",
    "                            category[logiccategory[row[\"ID\"]]] = [0, 0]\n",
    "                        \n",
    "                        # print(row[\"model_bruteforce\"])\n",
    "\n",
    "                        category[logiccategory[row[\"ID\"]]][1-int(row[\"model_bruteforce\"])] += 1\n",
    "\n",
    "                    if (row['ID'] < 50):\n",
    "                        correctnessDiff.append(int(row[\"correctness\"]))\n",
    "                    # count += 1\n",
    "\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # print(\"Error:\", e)\n",
    "                    # print(row[\"model_bruteforce\"])\n",
    "                    pass\n",
    "            # print(category)\n",
    "            \n",
    "            print(f\"Model: {model}, Test: {test}, Prompt: {prompt}\", \n",
    "                #   print(category),\n",
    "                  np.round(100*bfarray.flatten()/np.sum(bfarray), 1), np.sum(bfarray), \n",
    "                  \"Correctness:\", str(np.round(np.mean(correctness), 3)) + \"/\" + str(np.round(np.mean(correctnessDiff), 3)), \n",
    "                #   \"Difficulty (BF/NBF):\", np.round(np.mean(bfdiff), 2), np.round(np.mean(nbfdiff), 2), \n",
    "                #   \"Popularity (BF/NBF):\", np.round(np.mean(bfpop), 2), np.round(np.mean(nbfpop), 2),\n",
    "                  \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ed10e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Solution Math:\n",
      "Difficulty (BF/NBF): 2.83 2.8\n",
      "Popularity (BF/NBF): 2.32 2.33\n",
      "Human Solution Logic:\n",
      "Difficulty (BF/NBF): 2.7 2.65\n",
      "Popularity (BF/NBF): 2.37 2.51\n"
     ]
    }
   ],
   "source": [
    "print(\"Human Solution Math:\")\n",
    "totaldiffbf = []\n",
    "totaldiffnbf = []\n",
    "totalpopbf = []\n",
    "totalpopnbf = []\n",
    "for i in range(250):\n",
    "    if (humanbruteforcemath[i] == 1):\n",
    "        totaldiffbf.append(mathdifficulty[i])\n",
    "        totalpopbf.append(mathpopularity[i])\n",
    "    else:\n",
    "        totaldiffnbf.append(mathdifficulty[i])\n",
    "        totalpopnbf.append(mathpopularity[i])\n",
    "print(\"Difficulty (BF/NBF):\", np.round(np.mean(totaldiffbf), 2), np.round(np.mean(totaldiffnbf), 2))\n",
    "print(\"Popularity (BF/NBF):\", np.round(np.mean(totalpopbf), 2), np.round(np.mean(totalpopnbf), 2))\n",
    "print(\"Human Solution Logic:\")\n",
    "totaldiffbf = []\n",
    "totaldiffnbf = []\n",
    "totalpopbf = []\n",
    "totalpopnbf = []\n",
    "for i in range(250):\n",
    "    if (humanbruteforcelogic[i] == 1):\n",
    "        totaldiffbf.append(logicdifficulty[i])\n",
    "        totalpopbf.append(logicpopularity[i])\n",
    "    else:\n",
    "        totaldiffnbf.append(logicdifficulty[i])\n",
    "        totalpopnbf.append(logicpopularity[i])\n",
    "print(\"Difficulty (BF/NBF):\", np.round(np.mean(totaldiffbf), 2), np.round(np.mean(totaldiffnbf), 2))\n",
    "print(\"Popularity (BF/NBF):\", np.round(np.mean(totalpopbf), 2), np.round(np.mean(totalpopnbf), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912abea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Brainteasers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
